{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bacb402",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.004911,
     "end_time": "2024-12-28T14:58:24.619220",
     "exception": false,
     "start_time": "2024-12-28T14:58:24.614309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/himanshu-nakrani/TMLC-Gen-ai-projects/blob/main/RAG_Retrieval_Optimization_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647bfe82",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-28T14:58:24.629061Z",
     "iopub.status.busy": "2024-12-28T14:58:24.628696Z",
     "iopub.status.idle": "2024-12-28T14:59:12.695001Z",
     "shell.execute_reply": "2024-12-28T14:59:12.693660Z"
    },
    "id": "au4wxc3KTtmr",
    "outputId": "fb556ff3-e748-420d-f779-e981267f04e5",
    "papermill": {
     "duration": 48.073324,
     "end_time": "2024-12-28T14:59:12.696937",
     "exception": false,
     "start_time": "2024-12-28T14:58:24.623613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-cohere\r\n",
      "  Downloading langchain_cohere-0.3.4-py3-none-any.whl.metadata (6.7 kB)\r\n",
      "Collecting langchain\r\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Collecting pdfminer.six\r\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Collecting chromadb\r\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting rank_bm25\r\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere)\r\n",
      "  Downloading cohere-5.13.4-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-cohere)\r\n",
      "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere)\r\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (2.1.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (2.9.2)\r\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (0.9.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\r\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.2.6-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.1)\r\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2)\r\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\r\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\r\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\r\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting posthog>=2.4.0 (from chromadb)\r\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\r\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\r\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\r\n",
      "Collecting pypika>=0.48.9 (from chromadb)\r\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\r\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\r\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\r\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\r\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\r\n",
      "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\n",
      "Collecting orjson>=3.9.12 (from chromadb)\r\n",
      "  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb)\r\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.8.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\r\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\r\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\r\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\r\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere)\r\n",
      "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\r\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\r\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\r\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.23.4)\r\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\r\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\r\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\r\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\r\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\r\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\r\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\r\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\r\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\r\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.27->langchain-cohere)\r\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\r\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\r\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\r\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\r\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\r\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.24.7)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\r\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.6.1)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\r\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\r\n",
      "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (3.23.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\r\n",
      "Downloading langchain_cohere-0.3.4-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\r\n",
      "Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cohere-5.13.4-py3-none-any.whl (250 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\r\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\r\n",
      "Downloading langsmith-0.2.6-py3-none-any.whl (325 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.7/325.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\r\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\r\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\r\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\r\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\r\n",
      "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\r\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\r\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\r\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\r\n",
      "Building wheels for collected packages: pypika\r\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=66d735eb3c066dfae0b6c93e6a8576205526724b7a0e53c0a2c62f744eeb46c1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\r\n",
      "Successfully built pypika\r\n",
      "Installing collected packages: pypika, monotonic, durationpy, uvloop, types-requests, rank_bm25, python-dotenv, protobuf, parameterized, orjson, opentelemetry-util-http, mmh3, jsonpatch, humanfriendly, httpx-sse, httptools, h11, fastavro, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, requests-toolbelt, posthog, opentelemetry-proto, opentelemetry-api, httpcore, coloredlogs, pydantic-settings, pdfminer.six, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, httpx, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, langsmith, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain, chromadb, langchain-community, langchain-experimental, langchain-cohere\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\r\n",
      "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "google-cloud-bigtable 2.26.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "pandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\r\n",
      "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 chroma-hnswlib-0.7.6 chromadb-0.5.23 cohere-5.13.4 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 fastavro-1.10.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.1 httpx-sse-0.4.0 humanfriendly-10.0 jsonpatch-1.33 kubernetes-31.0.0 langchain-0.3.13 langchain-cohere-0.3.4 langchain-community-0.3.13 langchain-core-0.3.28 langchain-experimental-0.3.4 langchain-text-splitters-0.3.4 langsmith-0.2.6 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 orjson-3.10.12 parameterized-0.9.0 pdfminer.six-20240706 posthog-3.7.4 protobuf-5.29.2 pydantic-settings-2.7.0 pypika-0.48.9 python-dotenv-1.0.1 rank_bm25-0.2.2 requests-toolbelt-1.0.0 starlette-0.41.3 types-requests-2.32.0.20241016 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-cohere langchain pdfminer.six chromadb rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537bd557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:12.723849Z",
     "iopub.status.busy": "2024-12-28T14:59:12.723495Z",
     "iopub.status.idle": "2024-12-28T14:59:16.006282Z",
     "shell.execute_reply": "2024-12-28T14:59:16.005440Z"
    },
    "id": "37xhdj5YjIto",
    "outputId": "32dcd3fc-e683-40c5-eb03-67047d35ec7c",
    "papermill": {
     "duration": 3.298283,
     "end_time": "2024-12-28T14:59:16.008150",
     "exception": false,
     "start_time": "2024-12-28T14:59:12.709867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ[\"COHERE_API_KEY\"] = user_secrets.get_secret(\"COHERE_KEY\")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from pdfminer.high_level import extract_text as extract_text_pdf_miner\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486b3d",
   "metadata": {
    "id": "JsdpSFFRixIp",
    "papermill": {
     "duration": 0.012011,
     "end_time": "2024-12-28T14:59:16.033058",
     "exception": false,
     "start_time": "2024-12-28T14:59:16.021047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09296574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:16.059295Z",
     "iopub.status.busy": "2024-12-28T14:59:16.058776Z",
     "iopub.status.idle": "2024-12-28T14:59:16.396036Z",
     "shell.execute_reply": "2024-12-28T14:59:16.395011Z"
    },
    "id": "FGNwUhNvjbun",
    "papermill": {
     "duration": 0.352281,
     "end_time": "2024-12-28T14:59:16.397580",
     "exception": false,
     "start_time": "2024-12-28T14:59:16.045299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f6fcfcfaf463>:7: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereEmbeddings``.\n",
      "  embedding = CohereEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where the Chroma database will persist data\n",
    "persist_directory = \"/kaggle/working/\"\n",
    "\n",
    "# Initialize Cohere embeddings with the specified model\n",
    "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
    "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
    "embedding = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    user_agent=\"langchain\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d89e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:16.424175Z",
     "iopub.status.busy": "2024-12-28T14:59:16.423803Z",
     "iopub.status.idle": "2024-12-28T14:59:20.568276Z",
     "shell.execute_reply": "2024-12-28T14:59:20.567187Z"
    },
    "id": "-JzB6c1Rjcn4",
    "papermill": {
     "duration": 4.160024,
     "end_time": "2024-12-28T14:59:20.570324",
     "exception": false,
     "start_time": "2024-12-28T14:59:16.410300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_to_vectordb(file_path,source):\n",
    "  global bm25_retriever\n",
    "  # Loop through a list of PDF files to process\n",
    "  pages = []\n",
    "\n",
    "  for pdf_name in [file_path]:\n",
    "      # Open each PDF file in binary mode\n",
    "      with open(pdf_name, 'rb') as f:\n",
    "          # Extract text from the PDF using the extract_text_pdf_miner function\n",
    "          text = extract_text_pdf_miner(f)\n",
    "\n",
    "          # Clean the extracted text by removing newline characters and joining into a single string\n",
    "          cleaned_text = \" \".join(text.split(\"\\n\"))\n",
    "\n",
    "          # Initialize a list to store document chunks\n",
    "          docs = []\n",
    "\n",
    "          # Create a text splitter to divide the text into manageable chunks\n",
    "          # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
    "          splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
    "\n",
    "          # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
    "          for chunk in splitter.split_text(cleaned_text):\n",
    "              docs.append(Document(page_content=chunk, metadata={\"retrived_from\":source,\"source\": pdf_name}))\n",
    "              pages.append(Document(page_content=chunk, metadata={\"retrived_from\":source,\"source\": pdf_name}))\n",
    "      # Create a Chroma collection from the processed documents\n",
    "      # Use the specified persist directory and embedding model for storage and retrieval\n",
    "      if source == 1:\n",
    "        bm25_retriever = BM25Retriever.from_documents(pages)\n",
    "      else:\n",
    "        db = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            persist_directory=persist_directory,\n",
    "            embedding=embedding\n",
    "        )\n",
    "\n",
    "load_data_to_vectordb(file_path=\"/kaggle/input/100-ml-interview-questions-and-answers/100-Machine-Learning-Interview-Questions-and-Answers.pdf\",source=1)\n",
    "# load_data_to_vectordb(file_path=\"/content/1506.02640v5.pdf\",source=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5fac2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:20.597222Z",
     "iopub.status.busy": "2024-12-28T14:59:20.596858Z",
     "iopub.status.idle": "2024-12-28T14:59:20.601744Z",
     "shell.execute_reply": "2024-12-28T14:59:20.600693Z"
    },
    "id": "K85f6lm9i_d4",
    "outputId": "6077cfe0-ac9e-4251-98cc-ee9820a8ca19",
    "papermill": {
     "duration": 0.01982,
     "end_time": "2024-12-28T14:59:20.603198",
     "exception": false,
     "start_time": "2024-12-28T14:59:20.583378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of bm25 <class 'langchain_community.retrievers.bm25.BM25Retriever'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BM25 retriever\n",
    "bm25_retriever.k = 2  # Retrieve top 2 results\n",
    "print(\"type of bm25\", type(bm25_retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d24e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:20.629270Z",
     "iopub.status.busy": "2024-12-28T14:59:20.628946Z",
     "iopub.status.idle": "2024-12-28T14:59:21.758047Z",
     "shell.execute_reply": "2024-12-28T14:59:21.757045Z"
    },
    "id": "PogehneekHyI",
    "papermill": {
     "duration": 1.144142,
     "end_time": "2024-12-28T14:59:21.759926",
     "exception": false,
     "start_time": "2024-12-28T14:59:20.615784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-641eb3961709>:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "# Initialize retriever\n",
    "docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "retriever_chromadb = docsearch.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever_chromadb], weights=[0.3, 0.7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618d4284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:21.786755Z",
     "iopub.status.busy": "2024-12-28T14:59:21.786152Z",
     "iopub.status.idle": "2024-12-28T14:59:22.011670Z",
     "shell.execute_reply": "2024-12-28T14:59:22.010513Z"
    },
    "id": "ibG6dac4kRE4",
    "outputId": "903d85fc-17c5-4cd0-98fa-6c15f3486ed1",
    "papermill": {
     "duration": 0.240924,
     "end_time": "2024-12-28T14:59:22.013762",
     "exception": false,
     "start_time": "2024-12-28T14:59:21.772838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'retrived_from': 1, 'source': '/kaggle/input/100-ml-interview-questions-and-answers/100-Machine-Learning-Interview-Questions-and-Answers.pdf'}, page_content='same to the model that was trained on the remaining data set to get the required predictions.  2. K-Fold Cross-Validation: Here, the data is divided into k subsets so that every time, one  among the k subsets can be used as a validation set, and the other k-1 subsets are used as the training set  3. Stratified K-Fold Cross-Validation: It works on imbalanced data. 4. Leave-P-Out Cross-Validation: Here, we leave p data points out of the training data out of the n data points, then we use the n-p samples to train the model and p points for the validation set.  5. Differences Between The Bagging And Boosting Algorithms?  \\x0cBagging  Boosting  It is a method that merges the same type of predictions.  It is a method that merges the different types of predictions.  It decreases the variance, not the bias  It decreases the bias, not the variance.  Each and every model receives equal weight Models are weighed based on performance.  6. What Are Kernels In SVM? Can You List Some Popular Kernels Used In SVM?  The kernel is basically used to set mathematical functions that are used in the Support Vector Machine by providing the window to manipulate the data. Kernel Function is used to transform the training set of data so that a non-linear decision surface will be transformed to a linear equation in a bigger number of dimension spaces. Some of the popular kernels used in SVM are:  1. Polynomial kernel 2. Gaussian kernel 3. Gaussian radial basis function (RBF) 4. Laplace RBF kernel 5. Hyperbolic tangent kernel 6. Sigmoid kernel 7. Bessel function of the first kind Kernel 8. ANOVA radial basis kernel  7. Can You Explain The OOB Error?  An out-of-bag error called OBB error, also known as an out-of-bag estimate, is a technique to measure the prediction error of random forests, boosted decision trees. Bagging mainly uses subsampling with replacement to create the training samples for the model to learn from them.  8. Can You Differentiate Between K-Means And KNN Algorithms?  K-Means  KNN algorithms  It is unsupervised machine'),\n",
       " Document(metadata={'retrived_from': 1, 'source': '/kaggle/input/100-ml-interview-questions-and-answers/100-Machine-Learning-Interview-Questions-and-Answers.pdf'}, page_content='100 Machine Learning Interview Questions and Answers  1. Please Explain Machine Learning, Artificial Intelligence, And Deep Learning?  Machine learning is defined as a subset of Artificial Intelligence, and it contains the techniques which enable computers to sort things out from the data and deliver Artificial Intelligence applications. Artificial Intelligence (AI) is a branch of computer science that is mainly focused on building smart machines that can perform certain tasks that mainly require human intelligence. It is the venture to replicate or simulate human intelligence in machines. Deep learning can be defined as a class of machine learning algorithms in Artificial Intelligence that mainly uses multiple layers to cumulatively extract higher-level features from the given raw input.  2. How Difficult Is Machine Learning?  Machine Learning is huge and comprises a lot of things. Therefore, it will take more than six months to learn Machine Learning if you spend at least 6-7 hours per day. If you have good hands-on mathematical and analytical skills, then six months will be sufficient for you.  3. Can You Explain Kernel Trick In An SVM Algorithm?  A Kernel Trick is a method where the Non-Linear data is projected onto a bigger dimension space in order to make it easy to classify the data where it can be linearly divided by a plane.  4. Can You List Some Of The Popular Cross-Validation Techniques?  1. Holdout Method: This kind of technique works by removing the part of the training data  set and sending the same to the model that was trained on the remaining data set to get the required predictions.  2. K-Fold Cross-Validation: Here, the data is divided into k subsets so that every time, one  among the k subsets can be used as a validation set, and the other k-1 subsets are used as the training set  3. Stratified K-Fold Cross-Validation: It works on imbalanced data. 4. Leave-P-Out Cross-Validation: Here, we leave p data points out of the training data out of the n data points, then we use the n-p samples to')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What is k in KNN algorithm?\"\n",
    "\n",
    "# Retrieve relevant documents/products\n",
    "docs = ensemble_retriever.invoke(query)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebea1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:22.048250Z",
     "iopub.status.busy": "2024-12-28T14:59:22.047847Z",
     "iopub.status.idle": "2024-12-28T14:59:22.075414Z",
     "shell.execute_reply": "2024-12-28T14:59:22.074489Z"
    },
    "id": "C8ypvcYTkVOm",
    "outputId": "ead7363c-0661-4d93-8dca-903478760249",
    "papermill": {
     "duration": 0.047643,
     "end_time": "2024-12-28T14:59:22.077025",
     "exception": false,
     "start_time": "2024-12-28T14:59:22.029382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>retrieval_source</th>\n",
       "      <th>pdf_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>same to the model that was trained on the rema...</td>\n",
       "      <td>1</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 Machine Learning Interview Questions and A...</td>\n",
       "      <td>1</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content  retrieval_source  \\\n",
       "0  same to the model that was trained on the rema...                 1   \n",
       "1  100 Machine Learning Interview Questions and A...                 1   \n",
       "\n",
       "                                          pdf_source  \n",
       "0  /kaggle/input/100-ml-interview-questions-and-a...  \n",
       "1  /kaggle/input/100-ml-interview-questions-and-a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract and print only the page content from each document\n",
    "import pandas as pd\n",
    "\n",
    "retrieval_df = pd.DataFrame()\n",
    "\n",
    "page_content = []\n",
    "retrieval_source = []\n",
    "pdf_source = []\n",
    "\n",
    "for doc in docs:\n",
    "    page_content.append(doc.page_content)\n",
    "    retrieval_source.append(doc.metadata['retrived_from'])\n",
    "    pdf_source.append(doc.metadata['source'])\n",
    "\n",
    "retrieval_df['page_content'] = page_content\n",
    "retrieval_df['retrieval_source'] = retrieval_source\n",
    "retrieval_df['pdf_source'] = pdf_source\n",
    "\n",
    "retrieval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd218682",
   "metadata": {
    "id": "JP3qZGNzoMnw",
    "papermill": {
     "duration": 0.012526,
     "end_time": "2024-12-28T14:59:22.103374",
     "exception": false,
     "start_time": "2024-12-28T14:59:22.090848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Re Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c44ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:22.130882Z",
     "iopub.status.busy": "2024-12-28T14:59:22.130519Z",
     "iopub.status.idle": "2024-12-28T14:59:22.257022Z",
     "shell.execute_reply": "2024-12-28T14:59:22.255967Z"
    },
    "id": "-927YuLknYLN",
    "papermill": {
     "duration": 0.142262,
     "end_time": "2024-12-28T14:59:22.258879",
     "exception": false,
     "start_time": "2024-12-28T14:59:22.116617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory where the Chroma database will persist data\n",
    "persist_directory = \"/kaggle/working/\"\n",
    "\n",
    "# Initialize Cohere embeddings with the specified model\n",
    "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
    "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
    "embedding = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    user_agent=\"langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0c1dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:22.286714Z",
     "iopub.status.busy": "2024-12-28T14:59:22.286218Z",
     "iopub.status.idle": "2024-12-28T14:59:27.189412Z",
     "shell.execute_reply": "2024-12-28T14:59:27.188579Z"
    },
    "id": "odGWYjzEoUCA",
    "papermill": {
     "duration": 4.918864,
     "end_time": "2024-12-28T14:59:27.191222",
     "exception": false,
     "start_time": "2024-12-28T14:59:22.272358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_to_vectordb(file_path,source):\n",
    "  # Loop through a list of PDF files to process\n",
    "\n",
    "  for pdf_name in [file_path]:\n",
    "      # Open each PDF file in binary mode\n",
    "      with open(pdf_name, 'rb') as f:\n",
    "          # Extract text from the PDF using the extract_text_pdf_miner function\n",
    "          text = extract_text_pdf_miner(f)\n",
    "\n",
    "          # Clean the extracted text by removing newline characters and joining into a single string\n",
    "          cleaned_text = \" \".join(text.split(\"\\n\"))\n",
    "\n",
    "          # Initialize a list to store document chunks\n",
    "          docs = []\n",
    "\n",
    "          # Create a text splitter to divide the text into manageable chunks\n",
    "          # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
    "          splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
    "\n",
    "          # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
    "          for chunk in splitter.split_text(cleaned_text):\n",
    "              docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
    "      # Create a Chroma collection from the processed documents\n",
    "      # Use the specified persist directory and embedding model for storage and retrieval\n",
    "\n",
    "      db = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            persist_directory=persist_directory,\n",
    "            embedding=embedding\n",
    "        )\n",
    "\n",
    "load_data_to_vectordb(file_path=\"/kaggle/input/100-ml-interview-questions-and-answers/100-Machine-Learning-Interview-Questions-and-Answers.pdf\",source=1)\n",
    "# load_data_to_vectordb(file_path=\"/content/1506.02640v5.pdf\",source=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1301f78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:27.219564Z",
     "iopub.status.busy": "2024-12-28T14:59:27.219097Z",
     "iopub.status.idle": "2024-12-28T14:59:27.223334Z",
     "shell.execute_reply": "2024-12-28T14:59:27.222364Z"
    },
    "id": "j0RFeoW6o6bf",
    "papermill": {
     "duration": 0.019641,
     "end_time": "2024-12-28T14:59:27.224837",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.205196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613b733c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:27.251721Z",
     "iopub.status.busy": "2024-12-28T14:59:27.251334Z",
     "iopub.status.idle": "2024-12-28T14:59:27.261235Z",
     "shell.execute_reply": "2024-12-28T14:59:27.259948Z"
    },
    "id": "8Shje5B2o1tQ",
    "papermill": {
     "duration": 0.025068,
     "end_time": "2024-12-28T14:59:27.262850",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.237782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faaf29a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:27.290282Z",
     "iopub.status.busy": "2024-12-28T14:59:27.289917Z",
     "iopub.status.idle": "2024-12-28T14:59:27.394494Z",
     "shell.execute_reply": "2024-12-28T14:59:27.393379Z"
    },
    "id": "xYXIYoBLqNmZ",
    "outputId": "c953fc8c-a6aa-47bd-df58-ab0075f1ebee",
    "papermill": {
     "duration": 0.120058,
     "end_time": "2024-12-28T14:59:27.395993",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.275935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-15e4bf0c64e3>:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  non_rerank_df = non_rerank_df._append(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>source</th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used to examine the association of independent...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.257310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>same to the model that was trained on the rema...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.170640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kernel 7. Bessel function of the first kind Ke...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.148103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  used to examine the association of independent...   \n",
       "1  same to the model that was trained on the rema...   \n",
       "2  kernel 7. Bessel function of the first kind Ke...   \n",
       "\n",
       "                                              source  relevance_score  \n",
       "0  /kaggle/input/100-ml-interview-questions-and-a...         0.257310  \n",
       "1  /kaggle/input/100-ml-interview-questions-and-a...         0.170640  \n",
       "2  /kaggle/input/100-ml-interview-questions-and-a...         0.148103  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame with specified columns\n",
    "non_rerank_df = pd.DataFrame(columns=['Text', 'source', 'relevance_score'])\n",
    "\n",
    "# Perform similarity search using a preconfigured document search tool\n",
    "# This retrieves the top 3 documents based on relevance to the query\n",
    "res_docs = docsearch.similarity_search_with_relevance_scores(\"What is k in KNN algorithm?\", k=3)\n",
    "\n",
    "# Loop through the retrieved documents and populate the DataFrame\n",
    "for doc in res_docs:\n",
    "    non_rerank_df = non_rerank_df._append(\n",
    "        {\n",
    "            'Text': doc[0].page_content,  # Extract the page content (text) from the document\n",
    "            'source': doc[0].metadata['source'],  # Extract the source metadata\n",
    "            'relevance_score': doc[1]  # Extract the relevance score\n",
    "        },\n",
    "        ignore_index=True  # Ensure the new row is appended without overwriting existing rows\n",
    "    )\n",
    "\n",
    "# Display the first 3 rows of the DataFrame\n",
    "non_rerank_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45113788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:27.423565Z",
     "iopub.status.busy": "2024-12-28T14:59:27.423171Z",
     "iopub.status.idle": "2024-12-28T14:59:27.516037Z",
     "shell.execute_reply": "2024-12-28T14:59:27.515012Z"
    },
    "id": "LooPA8hZo8mX",
    "papermill": {
     "duration": 0.108934,
     "end_time": "2024-12-28T14:59:27.518075",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.409141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-f983c412f7af>:2: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  compressor = CohereRerank()\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize the reranker for document compression\n",
    "compressor = CohereRerank()\n",
    "# CohereRerank is a model or tool designed to rerank documents based on their relevance.\n",
    "\n",
    "# Create a ContextualCompressionRetriever for improved document retrieval\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,               # Use the reranker as the base compression mechanism\n",
    "    base_retriever=docsearch.as_retriever()   # Use the existing document search tool as the base retriever\n",
    ")\n",
    "# The ContextualCompressionRetriever combines the base retriever's results with reranking\n",
    "# to provide more contextually relevant and concise results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "177a3ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T14:59:27.546298Z",
     "iopub.status.busy": "2024-12-28T14:59:27.545982Z",
     "iopub.status.idle": "2024-12-28T14:59:27.793392Z",
     "shell.execute_reply": "2024-12-28T14:59:27.792279Z"
    },
    "id": "ZB2e0L15pCc3",
    "outputId": "4b291f6d-3748-48d4-9fac-aed9e40292ec",
    "papermill": {
     "duration": 0.263171,
     "end_time": "2024-12-28T14:59:27.795042",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.531871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-ed8f874a587e>:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(\"What is k in KNN algorithm?\")\n",
      "<ipython-input-15-ed8f874a587e>:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  source_df = source_df._append(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>source</th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used to examine the association of independent...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.922622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kernel 7. Bessel function of the first kind Ke...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.784319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>same to the model that was trained on the rema...</td>\n",
       "      <td>/kaggle/input/100-ml-interview-questions-and-a...</td>\n",
       "      <td>0.528001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  used to examine the association of independent...   \n",
       "1  kernel 7. Bessel function of the first kind Ke...   \n",
       "2  same to the model that was trained on the rema...   \n",
       "\n",
       "                                              source  relevance_score  \n",
       "0  /kaggle/input/100-ml-interview-questions-and-a...         0.922622  \n",
       "1  /kaggle/input/100-ml-interview-questions-and-a...         0.784319  \n",
       "2  /kaggle/input/100-ml-interview-questions-and-a...         0.528001  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame with specified columns\n",
    "source_df = pd.DataFrame(columns=['Text', 'source', 'relevance_score'])\n",
    "\n",
    "# Retrieve compressed documents relevant to the query using the contextual compression retriever\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"What is k in KNN algorithm?\")\n",
    "\n",
    "# Loop through the first 3 compressed documents and populate the DataFrame\n",
    "for i in range(3):\n",
    "    source_df = source_df._append(\n",
    "        {\n",
    "            'Text': compressed_docs[i].page_content,  # Extract the content of the document\n",
    "            'source': compressed_docs[i].metadata['source'],  # Extract the source information\n",
    "            'relevance_score': compressed_docs[i].metadata['relevance_score']  # Extract the relevance score\n",
    "        },\n",
    "        ignore_index=True  # Ensure the new row is appended without overwriting existing rows\n",
    "    )\n",
    "\n",
    "# Display the first 3 rows of the DataFrame\n",
    "source_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e5e5c",
   "metadata": {
    "id": "iaYcHLO3suLs",
    "papermill": {
     "duration": 0.013029,
     "end_time": "2024-12-28T14:59:27.821503",
     "exception": false,
     "start_time": "2024-12-28T14:59:27.808474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see the last result is different and the relevance scores are also different but they are according to the relevance model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6388915,
     "sourceId": 10319347,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.553609,
   "end_time": "2024-12-28T14:59:29.057777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T14:58:22.504168",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
